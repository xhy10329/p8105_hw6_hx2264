---
title: "p8105_hw6_hx2264"
author: "Hongyao Xie"
date: "November 20, 2018"
output: github_document
---

## Problem 1

```{r echo = FALSE}
library(tidyverse)
library(gridExtra)
library(HH)
library(leaps)
library(ISLR)
library(glmnet)
library(modelr)
```

**Create a city_state variable (e.g. “Baltimore, MD”), and a binary variable indicating whether the homicide is solved.**

```{r}
# Create a city_state variable
homic <-  read_csv("./data/homicide-data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(city_state = paste(city, state, sep = ",")) 

# Create a binary variable
homic$hmc_resolve <- ifelse(homic$disposition == "Closed by arrest", 1, 0)
```

**Omit cities Dallas, TX; Phoenix, AZ; Kansas City, MO; and Tulsa, AL. Modifiy victim_race to have categories white and non-white, with white as the reference category. Be sure that victim_age is numeric.**

```{r}
homic <- homic %>% 
  filter(!city_state %in% c("Dallas,TX","Phoenix,AZ", "Kansas City,MO", "Tulsa,AL")) %>%
  filter(victim_race != "Unknown") %>% 
  mutate(victim_race = fct_relevel(ifelse(victim_race == "White", "white", "non-white"), "white")) %>%
  mutate(victim_age = as.numeric(victim_age))
```

**For the city of Baltimore, MD, use the glm function to fit a logistic regression with resolved vs unresolved as the outcome and victim age, sex and race (as just defined) as predictors. Save the output of  glm as an R object; apply the broom::tidy to this object**

```{r}
baltm <- homic %>% 
  filter(city_state == "Baltimore,MD")

baltm_glm <- glm(hmc_resolve ~ victim_age + victim_sex + victim_race, data = baltm, family = binomial()) %>% 
  broom::tidy()
baltm_glm
```

**Obtain the estimate and confidence interval of the adjusted odds ratio for solving homicides comparing non-white victims to white victims keeping all other variables fixed**

```{r}
baltm_glm <- baltm_glm %>% 
  mutate(or = exp(estimate),
         confint_low = exp(estimate - std.error*1.96),
         confint_up = exp(estimate + std.error*1.96)) %>% 
  dplyr::select(term, estimate, or, confint_low, confint_up) %>% 
  knitr::kable(digits = 2)
baltm_glm
```

The adjusted odds ratio for solving homicides comparing non-white victims to white victims 0.44, which indicates the odds of solving homicides with non-white victims is 0.44 times the odds of solving homicides with white victims.

The 95% confidence interval is (0.31, 0.62), which means we are 95% confident that the true odds ratio falls in this interval.

**Run glm for each of the cities in your dataset, and extract the adjusted odds ratio (and CI) for solving homicides comparing non-white victims to white victims**

```{r}
homic_city <- homic %>% 
  group_by(city_state) %>% 
  nest()

homic_city$glm_model <- map(homic_city$data, ~broom::tidy(glm(hmc_resolve ~ victim_age + victim_sex + victim_race, data = .x, family = binomial()))) 

homic_city <- homic_city %>% 
  dplyr::select(city_state, glm_model) %>% 
  unnest() %>%
  filter(term == "victim_racenon-white") %>% 
  mutate(or = exp(estimate),
         confint_low = exp(estimate - std.error*1.96),
         confint_up = exp(estimate + std.error*1.96)) %>% 
  dplyr::select(city_state, term, estimate, or, confint_low, confint_up)
homic_city
```

**Create a plot that shows the estimated ORs and CIs for each city. Organize cities according to estimated OR, and comment on the plot**

```{r}
homic_city_plot <- homic_city %>%
  mutate(city_state = fct_reorder(city_state, or)) %>% 
  ggplot(aes(x = city_state, y = or)) +
  geom_point() +
  geom_errorbar(mapping = aes(ymin = confint_low, ymax = confint_up)) + 
  labs(
    title = "The estimated odds ratio and confidence interval for solving homicides comparing non-white victims to white victims in each city",
    x = "City, state",
    y = "Odds ratio"
  ) +
  theme(axis.text.x = element_text(angle = 90, size = 8),
        title = element_text(size = 6)) 

homic_city_plot
```

Most cities have estimated odds ratio below 1, which implies in most cities the odds of solving homicides with non-white victims is lower than the odds of solving homicides with white victims.


## Problem 2

```{r}
# Load data and clean up NAs, convert some variables from numeric to factor
bw <- read_csv("./data/birthweight.csv") %>% 
  janitor::clean_names() %>% 
  na.omit() %>% 
  mutate(babysex = as.factor(babysex),
         frace = as.factor(frace),
         malform = as.factor(malform),
         mrace = as.factor(mrace))
```

**Propose a regression model for birthweight.**

```{r}
# A function to compare some criterias for model size selection and variable membership
# Use exhaustive/backward/forward/sequential replacement methods to choose optimal model size
summary_metrics <- NULL
which_all <- list()
for (my_mthd in c("exhaustive", "backward", "forward", "seqrep") ) {
  rs_res <- regsubsets(bwt ~ .,bw,method = my_mthd,nvmax = 19)
  summ_res <- summary(rs_res)
  which_all[[my_mthd]] <- summ_res$which
  for (metric_name in c("rsq","rss","adjr2","cp","bic") ) {
    summary_metrics <- rbind(summary_metrics,
      data.frame(method = my_mthd,metric = metric_name,
                nvars = 1:length(summ_res[[metric_name]]),
                value = summ_res[[metric_name]]))
  }
}

# Plot R-square/adjusted R-square/RSS/Cp/BIC value to choose optimal model size
ggplot(summary_metrics,aes(x = nvars,y = value,shape = method, colour = method)) + 
  geom_path() + 
  geom_point() + 
  facet_wrap(~ metric,scales = "free") +   
  theme(legend.position = "top")

old.par <- par(mfrow = c(2,2),ps = 16,mar = c(5,7,2,1))

# Plot variable memberships for each model selection method
for (my_mthd in names(which_all) ) {
  image(1:nrow(which_all[[my_mthd]]),
        1:ncol(which_all[[my_mthd]]),
        which_all[[my_mthd]],xlab = "N(vars)",ylab = "",
        xaxt = "n",yaxt = "n",breaks = c(-0.5,0.5,1.5),
        col = c("white","gray"),main = my_mthd)
  axis(1,1:nrow(which_all[[my_mthd]]),rownames(which_all[[my_mthd]]))
  axis(2,1:ncol(which_all[[my_mthd]]),colnames(which_all[[my_mthd]]),las = 2)
}
```

**Describe your modeling process.**

Here I used regsubsets function from leaps packages. This function selected models by exhaustive search, forward or backward stepwise, or sequential replacement once specified. I selected out R-square/adjusted R-square/RSS/Cp/BIC values for each model size and made ggplot to visualize the values. After visualization, we can see when variables are 8, the model tends to be "optimal" (to achieve bias/variance balance).

Then we want to know which 8 variables were selected by the four methods from regsubsets function. I used image function to create a grid of rectagle plots to indicate whether each variable was included or not. After all these steps, we can see the variable memberships for each method. 

Finally, we chose smoken, ppbmi, gaweeks, delwt, blength, bhead for our model. Although we see specific mother race and father race (mrace = 2 and frace = 4) also had significant influence on birth weight from model selection process, we did not include these two variables here, since other levels of mrace and frace didn't show significant differences (but we can surely stratify these two variables to fit the model).


```{r}
# Final model: variable = 6, including smoken, ppbmi, gaweeks, delwt, blength, bhead
bw_lm <- lm(bwt ~ smoken + ppbmi + gaweeks + delwt + blength + bhead, data = bw)
summary(bw_lm)
```

**Show a plot of model residuals against fitted values – use add_predictions and add_residuals in making this plot**

```{r}
bw %>% 
  add_residuals(bw_lm) %>% 
  add_predictions(bw_lm) %>% 
  ggplot(aes(x = pred, y = resid)) + 
  geom_point(alpha = 0.2) +
  labs(
     title = "Model residuals against fitted values",
     x = "Fitted values",
     y = "Residuals"
   )
```

**Compare your model to two others**

```{r}
# One using length at birth and gestational age as predictors (main effects only)
bw_model1 <- lm(bwt ~ blength + gaweeks, data = bw)
summary(bw_model1)
```

```{r}
# One using head circumference, length, sex, and all interactions (including the three-way interaction) between these
bw_model2 <- lm(bwt ~ bhead + blength + babysex + bhead*babysex + bhead*blength + blength*babysex + bhead*babysex*blength, data = bw)
summary(bw_model2)
```

**Make this comparison in terms of the cross-validated prediction error**

```{r}
bw_cv <- bw %>% 
  crossv_mc(100)

cv_models_compare <- 
  bw_cv %>% 
   mutate(my_model = map(train, ~lm(bwt ~ smoken + ppbmi + gaweeks + delwt + blength + bhead, data = .)),
         model_1 = map(train, ~lm(bwt ~ blength + gaweeks, data = .)),
         model_2 = map(train, ~lm(bwt ~ bhead + blength + babysex + bhead*babysex + bhead*blength + blength*babysex + bhead*babysex*blength, data = .)))%>% 
   mutate(rmse_my_model = map2_dbl(my_model, test, ~rmse(.x, .y)),
         rmse_model_1 = map2_dbl(model_1, test, ~rmse(.x, .y)),
         rmse_model_2 = map2_dbl(model_2, test, ~rmse(.x, .y)))

cv_models_compare %>% 
  dplyr::select(starts_with('rmse')) %>% 
  gather(key = model, value = rmse, rmse_my_model:rmse_model_2) %>% 
  mutate(model = str_replace(model, "rmse_", '')) %>% 
  ggplot(aes(x = model, y = rmse)) +
  geom_violin() + 
  labs(
     title = "Prediction error distribution for each candidate model",
     x = "Model",
     y = "RMSE"
   )
```

As we can see from the violin plot, my model has the lowest RMSE and the model 1 has the highest RMSE, which implies my model and model 2 do a better job regarding prediction accuracy than model 1. This makes sense because model 1 only includes two variables(length at birth and gestational age) to predict birth weight, which does not consider other factors that have significant effect on birth weight as well.













































